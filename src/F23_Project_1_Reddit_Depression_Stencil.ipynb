{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jFvbbC6VtZm"
      },
      "source": [
        "Detecting Symptoms of Depression on Reddit\n",
        "Link to the paper: https://dl.acm.org/doi/pdf/10.1145/3578503.3583621\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJgzDthiSxu9",
        "outputId": "9604ea13-3e2f-43dd-8798-09eb0fe824d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting happiestfuntokenizing\n",
            "  Downloading happiestfuntokenizing-0.0.7.tar.gz (6.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: happiestfuntokenizing\n",
            "  Building wheel for happiestfuntokenizing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for happiestfuntokenizing: filename=happiestfuntokenizing-0.0.7-py3-none-any.whl size=6710 sha256=2bb0a35a1b227cd2068eef56040de95d485abaff651ca41daa4768cb29f68869\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/c9/4d/310f0c60855eb7b428558f29d93cf464dbb64c1b8628753395\n",
            "Successfully built happiestfuntokenizing\n",
            "Installing collected packages: happiestfuntokenizing\n",
            "Successfully installed happiestfuntokenizing-0.0.7\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.3.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install happiestfuntokenizing\n",
        "! pip install transformers\n",
        "! pip install joblib\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WklAuXIQf7YN",
        "outputId": "1aff868b-3be1-440d-cdbf-21e1e448c12b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoBxKQ_OVl-j",
        "outputId": "0f37b44d-7793-42e7-a56a-b29ee7a15606"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_validate, cross_val_score, KFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from happiestfuntokenizing.happiestfuntokenizing import Tokenizer\n",
        "from google.colab import drive\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from collections import Counter\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "import torch\n",
        "import re\n",
        "from joblib import dump\n",
        "import joblib\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "FILEPATH = '/content/drive/MyDrive/Colab Notebooks/nlp/student.pkl'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcMOTL7mV9T9"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "icnzto8GWFlb"
      },
      "outputs": [],
      "source": [
        "def load(file_path):\n",
        "  \"\"\"Load pickles\"\"\"\n",
        "  return pd.read_pickle(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Wpw9kJiras4B"
      },
      "outputs": [],
      "source": [
        "def dataset_generation(df, mental_health_subreddits):\n",
        "    \"\"\"Build control and symptom datasets.\"\"\"\n",
        "    # Set a time threshold of 180 days in seconds\n",
        "    time_threshold=180*24*60*60\n",
        "    # Filter the DataFrame to include only posts from mental health subreddits\n",
        "    symptom_posts = df[df['subreddit'].isin(mental_health_subreddits)]\n",
        "\n",
        "    earliest_mental_health_post = symptom_posts.groupby('author')['created_utc'].min()\n",
        "\n",
        "    # Filter control posts to only include those that are at least 180 days older than the earliest mental health post\n",
        "    # and are not from mental health subreddits\n",
        "    control_posts = pd.merge(df, earliest_mental_health_post, on='author')\n",
        "    control_posts = control_posts[(control_posts['created_utc_x'] <= control_posts['created_utc_y'] - time_threshold) &\n",
        "                                  (~control_posts['subreddit'].isin(mental_health_subreddits))]\n",
        "    # Remove posts where the author is '[deleted]' to ensure data quality\n",
        "    control_posts = control_posts[control_posts['author'] != '[deleted]']\n",
        "    symptom_posts = symptom_posts[symptom_posts['author'] != '[deleted]']\n",
        "\n",
        "    return control_posts, symptom_posts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ohOK3wCdWpnA"
      },
      "outputs": [],
      "source": [
        "# List of depression subreddits in the paper\n",
        "depression_subreddits = [\"Anger\",\n",
        "    \"anhedonia\", \"DeadBedrooms\",\n",
        "    \"Anxiety\", \"AnxietyDepression\", \"HealthAnxiety\", \"PanicAttack\",\n",
        "    \"DecisionMaking\", \"shouldi\",\n",
        "    \"bingeeating\", \"BingeEatingDisorder\", \"EatingDisorders\", \"eating_disorders\", \"EDAnonymous\",\n",
        "    \"chronicfatigue\", \"Fatigue\",\n",
        "    \"ForeverAlone\", \"lonely\",\n",
        "    \"cry\", \"grief\", \"sad\", \"Sadness\",\n",
        "    \"AvPD\", \"SelfHate\", \"selfhelp\", \"socialanxiety\", \"whatsbotheringyou\",\n",
        "    \"insomnia\", \"sleep\",\n",
        "    \"cfs\", \"ChronicPain\", \"Constipation\", \"EssentialTremor\", \"headaches\", \"ibs\", \"tinnitus\",\n",
        "    \"AdultSelfHarm\", \"selfharm\", \"SuicideWatch\",\n",
        "    \"Guilt\", \"Pessimism\", \"selfhelp\", \"whatsbotheringyou\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "otGQHJbRSqI1"
      },
      "outputs": [],
      "source": [
        "symptom_subreddit = {'Anger': ['Anger'],\n",
        "                     'Anhedonia': [\"anhedonia\", \"DeadBedrooms\"],\n",
        "                     'Anxiety': [\"Anxiety\", \"AnxietyDepression\", \"HealthAnxiety\", \"PanicAttack\"],\n",
        "                     'Disordered eating': [\"bingeeating\", \"BingeEatingDisorder\", \"EatingDisorders\", \"eating_disorders\", \"EDAnonymous\"],\n",
        "                     'Loneliness': [\"ForeverAlone\", \"lonely\"],\n",
        "                     'Sad mood': [\"cry\", \"grief\", \"sad\", \"Sadness\"],\n",
        "                     'Self-loathing': [\"AvPD\", \"SelfHate\", \"selfhelp\", \"socialanxiety\", \"whatsbotheringyou\"],\n",
        "                     'Sleep problem': [\"insomnia\", \"sleep\"],\n",
        "                     'Somatic complaint': [\"cfs\", \"ChronicPain\", \"Constipation\", \"EssentialTremor\", \"headaches\", \"ibs\", \"tinnitus\"],\n",
        "                     'Worthlessness': [\"Guilt\", \"Pessimism\", \"selfhelp\", \"whatsbotheringyou\"]\n",
        "                     }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MWGVUju_WxuP"
      },
      "outputs": [],
      "source": [
        "\n",
        "def mask_urls(tokens):\n",
        "  url_pattern = re.compile(r'http[s]?://\\S+')\n",
        "  url = ['[URL]' if url_pattern.match(token) else token for token in tokens]\n",
        "  return url\n",
        "def tokenize(data):\n",
        "    \"\"\"Tokenize\"\"\"\n",
        "\n",
        "    tokenizer = Tokenizer(preserve_case=False)\n",
        "\n",
        "    # Tokenize\n",
        "    data['tokenized_text'] = data['text'].apply(lambda x: tokenizer.tokenize(x))\n",
        "    #mask URLs\n",
        "    data['tokenized_text'] = data['tokenized_text'].apply(mask_urls)\n",
        "\n",
        "    # Convert tokens back to string\n",
        "    data['tokenized_text_str'] = data['tokenized_text'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Q3j9z7UuW3eG"
      },
      "outputs": [],
      "source": [
        "def stop_words(texts,n_top_words):\n",
        "  \"\"\"Find top 100 words from Reddit dataset to use as stop words\"\"\"\n",
        "  all_words = [word for text in texts for word in text.split()]\n",
        "  word_counts = Counter(all_words)\n",
        "\n",
        "  common_words = [word for word, count in word_counts.most_common(n_top_words)]\n",
        "  return common_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4I37U1SXAEZ"
      },
      "source": [
        "## Reddit Topics with LDA\n",
        "\n",
        " - Don't use MALLET (as the paper does), use some other LDA implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xf3surfWXH-q"
      },
      "outputs": [],
      "source": [
        "# TODO: Your LDA code!\n",
        "\n",
        "\n",
        "def preprocess_and_apply_lda(data, n_components=200, n_top_words=100):\n",
        "    data = tokenize(df)\n",
        "    texts = data['tokenized_text_str'].tolist()\n",
        "    common_words =  stop_words(texts,n_top_words)\n",
        "    vectorizer = CountVectorizer(stop_words=common_words)\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "\n",
        "    lda = LatentDirichletAllocation(n_components=n_components, random_state=0)\n",
        "    lda.fit(X)\n",
        "    doc_topic_distributions = lda.transform(X)\n",
        "\n",
        "    return lda,doc_topic_distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0-97hsVXNkF"
      },
      "source": [
        "## RoBERTa Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "blx1SWVMXYDp"
      },
      "outputs": [],
      "source": [
        "# TODO: Your RoBERTa code!\n",
        "def get_embedding(texts,tokenizer,device, model,layer_num=10):\n",
        "    encoded_input = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoded_input)\n",
        "        hidden_states = outputs.hidden_states\n",
        "    target_layer = hidden_states[layer_num]\n",
        "    mean_embedding = torch.mean(target_layer, dim=1)\n",
        "    return mean_embedding.cpu().numpy()\n",
        "\n",
        "def roberta_embeddings(df, layer_num=10):\n",
        "    \"\"\"Generate embeddings for texts using RoBERTa.\"\"\"\n",
        "    model_name = 'roberta-base'\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "    model = RobertaModel.from_pretrained(model_name, output_hidden_states=True)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    embeddings = df['text'].apply(lambda x: get_embedding(x,tokenizer,device,model))\n",
        "    np.save('/content/drive/MyDrive/Colab Notebooks/nlp/embedding33.npy', embeddings)\n",
        "    return np.vstack(embeddings.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDWxuF2jXtwi"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YIrNqpChbbu4"
      },
      "outputs": [],
      "source": [
        "def rf(embeddings,control_posts, symptom_posts,df,symptom_subreddit):\n",
        "    \"\"\"random forest\"\"\"\n",
        "\n",
        "    train_scores_dict = {}\n",
        "    test_scores_dict = {}\n",
        "\n",
        "    # Iterate over each symptom and its corresponding subreddits\n",
        "    for symptom, subreddits in symptom_subreddit.items():\n",
        "        # Create a mask to filter out posts belonging to the current symptom's subreddits\n",
        "        symptom_mask = df['subreddit'].isin(subreddits)\n",
        "        # Extract embeddings for control and symptom posts\n",
        "        control_embeddings = embeddings[:len(control_posts)]\n",
        "        symptom_embeddings = embeddings[symptom_mask]\n",
        "        # Combine control and symptom embeddings and create labels (0 for control, 1 for symptom)\n",
        "        X = np.concatenate((control_embeddings, symptom_embeddings))\n",
        "        y = np.concatenate((np.zeros(len(control_embeddings)), np.ones(len(symptom_embeddings))))\n",
        "        rf_classifier = RandomForestClassifier()\n",
        "        cv = KFold(n_splits=5, shuffle=True)\n",
        "        results = cross_validate(rf_classifier, X=X, y=y, cv=cv, scoring='roc_auc', return_train_score=True)\n",
        "        #Store the training and testing scores\n",
        "        train_scores = results['train_score']\n",
        "        test_scores = results['test_score']\n",
        "        train_scores_dict[symptom] = results['train_score']\n",
        "        test_scores_dict[symptom] = results['test_score']\n",
        "\n",
        "\n",
        "    return train_scores_dict , test_scores_dict\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koTBPhcDXujb",
        "outputId": "4a5fe315-ed1c-4dc5-bb1f-110edfd97b02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Symptom              LDA Test Score  RoBERTa Test Score\n",
            "Anger                0.925           0.959          \n",
            "Anhedonia            0.961           0.966          \n",
            "Anxiety              0.912           0.959          \n",
            "Disordered eating    0.970           0.966          \n",
            "Loneliness           0.888           0.926          \n",
            "Sad mood             0.840           0.941          \n",
            "Self-loathing        0.876           0.939          \n",
            "Sleep problem        0.981           0.975          \n",
            "Somatic complaint    0.932           0.948          \n",
            "Worthlessness        0.772           0.932          \n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "  \"\"\"\n",
        "  Here's the basic structure of the main block! It should run\n",
        "  5-fold cross validation with random forest to evaluate your RoBERTa and LDA\n",
        "  performance.\n",
        "  \"\"\"\n",
        "  df = load(FILEPATH)\n",
        "  control_posts, symptom_posts = dataset_generation(df,depression_subreddits)\n",
        "  df = pd.concat([control_posts, symptom_posts])\n",
        "  lda_model,doc_topic_distributions = preprocess_and_apply_lda(df)\n",
        "  #dump(doc_topic_distributions, '/content/drive/MyDrive/Colab Notebooks/doc_topic_distributions.joblib')\n",
        "  #doc_topic_distributions = joblib.load('/content/drive/MyDrive/Colab Notebooks/doc_topic_distributions.joblib')\n",
        "  roberta = roberta_embeddings(df)\n",
        "  #embeddings =np.load('/content/drive/MyDrive/Colab Notebooks/nlp/embedding33.npy', allow_pickle=True)\n",
        "  #roberta = np.vstack(embeddings.tolist())\n",
        "  lda_train,lda_test = rf(doc_topic_distributions,control_posts, symptom_posts,df,symptom_subreddit)\n",
        "  roberta_train,roberta_test = rf(roberta,control_posts, symptom_posts,df,symptom_subreddit)\n",
        "\n",
        "  print(f\"{'Symptom':<20} {'LDA Test Score':<15} {'RoBERTa Test Score':<15}\")\n",
        "  for symptom in symptom_subreddit.keys():\n",
        "      lda_test_score = np.mean(lda_test[symptom])\n",
        "      roberta_test_score = np.mean(roberta_test[symptom])\n",
        "      print(f\"{symptom:<20} {lda_test_score:<15.3f} {roberta_test_score:<15.3f}\")\n",
        "\n",
        "main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
